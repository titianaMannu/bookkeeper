<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="it"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>EntryLogManagerForEntryLogPerLedger.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">Tests</a> &gt; <a href="../index.html" class="el_bundle">bookkeeper-server</a> &gt; <a href="index.source.html" class="el_package">org.apache.bookkeeper.bookie</a> &gt; <span class="el_source">EntryLogManagerForEntryLogPerLedger.java</span></div><h1>EntryLogManagerForEntryLogPerLedger.java</h1><pre class="source lang-java linenums">/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import static org.apache.bookkeeper.bookie.BookKeeperServerStats.CATEGORY_SERVER;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.ENTRYLOGGER_SCOPE;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.ENTRYLOGS_PER_LEDGER;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.NUM_LEDGERS_HAVING_MULTIPLE_ENTRYLOGS;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.NUM_OF_WRITE_ACTIVE_LEDGERS;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_EXPIRY;
import static org.apache.bookkeeper.bookie.BookKeeperServerStats.NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_MAXSIZE;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.cache.CacheBuilder;
import com.google.common.cache.CacheLoader;
import com.google.common.cache.LoadingCache;
import com.google.common.cache.RemovalCause;
import com.google.common.cache.RemovalListener;
import com.google.common.cache.RemovalNotification;

import io.netty.buffer.ByteBuf;

import java.io.File;
import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReferenceArray;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

import lombok.extern.slf4j.Slf4j;

import org.apache.bookkeeper.bookie.EntryLogger.BufferedLogChannel;
import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.stats.Counter;
import org.apache.bookkeeper.stats.OpStatsLogger;
import org.apache.bookkeeper.stats.StatsLogger;
import org.apache.bookkeeper.stats.annotations.StatsDoc;
import org.apache.bookkeeper.util.IOUtils;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.collections.ConcurrentLongHashMap;
import org.apache.commons.lang3.mutable.MutableInt;

<span class="nc" id="L72">@Slf4j</span>
class EntryLogManagerForEntryLogPerLedger extends EntryLogManagerBase {

    static class BufferedLogChannelWithDirInfo {
        private final BufferedLogChannel logChannel;
<span class="nc" id="L77">        volatile boolean ledgerDirFull = false;</span>

<span class="nc" id="L79">        private BufferedLogChannelWithDirInfo(BufferedLogChannel logChannel) {</span>
<span class="nc" id="L80">            this.logChannel = logChannel;</span>
<span class="nc" id="L81">        }</span>

        private boolean isLedgerDirFull() {
<span class="nc" id="L84">            return ledgerDirFull;</span>
        }

        private void setLedgerDirFull(boolean ledgerDirFull) {
<span class="nc" id="L88">            this.ledgerDirFull = ledgerDirFull;</span>
<span class="nc" id="L89">        }</span>

        BufferedLogChannel getLogChannel() {
<span class="nc" id="L92">            return logChannel;</span>
        }
    }

    class EntryLogAndLockTuple {
        private final Lock ledgerLock;
        private BufferedLogChannelWithDirInfo entryLogWithDirInfo;

<span class="nc" id="L100">        private EntryLogAndLockTuple(long ledgerId) {</span>
<span class="nc" id="L101">            int lockIndex = MathUtils.signSafeMod(Long.hashCode(ledgerId), lockArrayPool.length());</span>
<span class="nc bnc" id="L102" title="All 2 branches missed.">            if (lockArrayPool.get(lockIndex) == null) {</span>
<span class="nc" id="L103">                lockArrayPool.compareAndSet(lockIndex, null, new ReentrantLock());</span>
            }
<span class="nc" id="L105">            ledgerLock = lockArrayPool.get(lockIndex);</span>
<span class="nc" id="L106">        }</span>

        private Lock getLedgerLock() {
<span class="nc" id="L109">            return ledgerLock;</span>
        }

        BufferedLogChannelWithDirInfo getEntryLogWithDirInfo() {
<span class="nc" id="L113">            return entryLogWithDirInfo;</span>
        }

        private void setEntryLogWithDirInfo(BufferedLogChannelWithDirInfo entryLogWithDirInfo) {
<span class="nc" id="L117">            this.entryLogWithDirInfo = entryLogWithDirInfo;</span>
<span class="nc" id="L118">        }</span>
    }

    @StatsDoc(
        name = ENTRYLOGGER_SCOPE,
        category = CATEGORY_SERVER,
        help = &quot;EntryLogger related stats&quot;
    )
    class EntryLogsPerLedgerCounter {

        @StatsDoc(
            name = NUM_OF_WRITE_ACTIVE_LEDGERS,
            help = &quot;Number of write active ledgers&quot;
        )
        private final Counter numOfWriteActiveLedgers;
        @StatsDoc(
            name = NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_EXPIRY,
            help = &quot;Number of write ledgers removed after cache expiry&quot;
        )
        private final Counter numOfWriteLedgersRemovedCacheExpiry;
        @StatsDoc(
            name = NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_MAXSIZE,
            help = &quot;Number of write ledgers removed due to reach max cache size&quot;
        )
        private final Counter numOfWriteLedgersRemovedCacheMaxSize;
        @StatsDoc(
            name = NUM_LEDGERS_HAVING_MULTIPLE_ENTRYLOGS,
            help = &quot;Number of ledgers having multiple entry logs&quot;
        )
        private final Counter numLedgersHavingMultipleEntrylogs;
        @StatsDoc(
            name = ENTRYLOGS_PER_LEDGER,
            help = &quot;The distribution of number of entry logs per ledger&quot;
        )
        private final OpStatsLogger entryLogsPerLedger;
        /*
         * ledgerIdEntryLogCounterCacheMap cache will be used to store count of
         * entrylogs as value for its ledgerid key. This cacheMap limits -
         * 'expiry duration' and 'maximumSize' will be set to
         * entryLogPerLedgerCounterLimitsMultFactor times of
         * 'ledgerIdEntryLogMap' cache limits. This is needed because entries
         * from 'ledgerIdEntryLogMap' can be removed from cache becasue of
         * accesstime expiry or cache size limits, but to know the actual number
         * of entrylogs per ledger, we should maintain this count for long time.
         */
        private final LoadingCache&lt;Long, MutableInt&gt; ledgerIdEntryLogCounterCacheMap;

<span class="nc" id="L165">        EntryLogsPerLedgerCounter(StatsLogger statsLogger) {</span>
<span class="nc" id="L166">            this.numOfWriteActiveLedgers = statsLogger.getCounter(NUM_OF_WRITE_ACTIVE_LEDGERS);</span>
<span class="nc" id="L167">            this.numOfWriteLedgersRemovedCacheExpiry = statsLogger</span>
<span class="nc" id="L168">                    .getCounter(NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_EXPIRY);</span>
<span class="nc" id="L169">            this.numOfWriteLedgersRemovedCacheMaxSize = statsLogger</span>
<span class="nc" id="L170">                    .getCounter(NUM_OF_WRITE_LEDGERS_REMOVED_CACHE_MAXSIZE);</span>
<span class="nc" id="L171">            this.numLedgersHavingMultipleEntrylogs = statsLogger.getCounter(NUM_LEDGERS_HAVING_MULTIPLE_ENTRYLOGS);</span>
<span class="nc" id="L172">            this.entryLogsPerLedger = statsLogger.getOpStatsLogger(ENTRYLOGS_PER_LEDGER);</span>

<span class="nc" id="L174">            ledgerIdEntryLogCounterCacheMap = CacheBuilder.newBuilder()</span>
<span class="nc" id="L175">                    .expireAfterAccess(entrylogMapAccessExpiryTimeInSeconds * entryLogPerLedgerCounterLimitsMultFactor,</span>
                            TimeUnit.SECONDS)
<span class="nc" id="L177">                    .maximumSize(maximumNumberOfActiveEntryLogs * entryLogPerLedgerCounterLimitsMultFactor)</span>
<span class="nc" id="L178">                    .removalListener(new RemovalListener&lt;Long, MutableInt&gt;() {</span>
                        @Override
                        public void onRemoval(RemovalNotification&lt;Long, MutableInt&gt; removedEntryFromCounterMap) {
<span class="nc bnc" id="L181" title="All 2 branches missed.">                            if ((removedEntryFromCounterMap != null)</span>
<span class="nc bnc" id="L182" title="All 2 branches missed.">                                    &amp;&amp; (removedEntryFromCounterMap.getValue() != null)) {</span>
<span class="nc" id="L183">                                synchronized (EntryLogsPerLedgerCounter.this) {</span>
<span class="nc" id="L184">                                    entryLogsPerLedger</span>
<span class="nc" id="L185">                                            .registerSuccessfulValue(removedEntryFromCounterMap.getValue().intValue());</span>
<span class="nc" id="L186">                                }</span>
                            }
<span class="nc" id="L188">                        }</span>
<span class="nc" id="L189">                    }).build(new CacheLoader&lt;Long, MutableInt&gt;() {</span>
                        @Override
                        public MutableInt load(Long key) throws Exception {
<span class="nc" id="L192">                            synchronized (EntryLogsPerLedgerCounter.this) {</span>
<span class="nc" id="L193">                                return new MutableInt();</span>
                            }
                        }
                    });
<span class="nc" id="L197">        }</span>

        private synchronized void openNewEntryLogForLedger(Long ledgerId, boolean newLedgerInEntryLogMapCache) {
<span class="nc" id="L200">            int numOfEntrylogsForThisLedger = ledgerIdEntryLogCounterCacheMap.getUnchecked(ledgerId).incrementAndGet();</span>
<span class="nc bnc" id="L201" title="All 2 branches missed.">            if (numOfEntrylogsForThisLedger == 2) {</span>
<span class="nc" id="L202">                numLedgersHavingMultipleEntrylogs.inc();</span>
            }
<span class="nc bnc" id="L204" title="All 2 branches missed.">            if (newLedgerInEntryLogMapCache) {</span>
<span class="nc" id="L205">                numOfWriteActiveLedgers.inc();</span>
            }
<span class="nc" id="L207">        }</span>

        private synchronized void removedLedgerFromEntryLogMapCache(Long ledgerId, RemovalCause cause) {
<span class="nc" id="L210">            numOfWriteActiveLedgers.dec();</span>
<span class="nc bnc" id="L211" title="All 2 branches missed.">            if (cause.equals(RemovalCause.EXPIRED)) {</span>
<span class="nc" id="L212">                numOfWriteLedgersRemovedCacheExpiry.inc();</span>
<span class="nc bnc" id="L213" title="All 2 branches missed.">            } else if (cause.equals(RemovalCause.SIZE)) {</span>
<span class="nc" id="L214">                numOfWriteLedgersRemovedCacheMaxSize.inc();</span>
            }
<span class="nc" id="L216">        }</span>

        /*
         * this is for testing purpose only. guava's cache doesnt cleanup
         * completely (including calling expiry removal listener) automatically
         * when access timeout elapses.
         *
         * https://google.github.io/guava/releases/19.0/api/docs/com/google/
         * common/cache/CacheBuilder.html
         *
         * If expireAfterWrite or expireAfterAccess is requested entries may be
         * evicted on each cache modification, on occasional cache accesses, or
         * on calls to Cache.cleanUp(). Expired entries may be counted by
         * Cache.size(), but will never be visible to read or write operations.
         *
         * Certain cache configurations will result in the accrual of periodic
         * maintenance tasks which will be performed during write operations, or
         * during occasional read operations in the absence of writes. The
         * Cache.cleanUp() method of the returned cache will also perform
         * maintenance, but calling it should not be necessary with a high
         * throughput cache. Only caches built with removalListener,
         * expireAfterWrite, expireAfterAccess, weakKeys, weakValues, or
         * softValues perform periodic maintenance.
         */
        @VisibleForTesting
        void doCounterMapCleanup() {
<span class="nc" id="L242">            ledgerIdEntryLogCounterCacheMap.cleanUp();</span>
<span class="nc" id="L243">        }</span>

        @VisibleForTesting
        ConcurrentMap&lt;Long, MutableInt&gt; getCounterMap() {
<span class="nc" id="L247">            return ledgerIdEntryLogCounterCacheMap.asMap();</span>
        }
    }

    private final AtomicReferenceArray&lt;Lock&gt; lockArrayPool;
    private final LoadingCache&lt;Long, EntryLogAndLockTuple&gt; ledgerIdEntryLogMap;
    /*
     * every time active logChannel is accessed from ledgerIdEntryLogMap
     * cache, the accesstime of that entry is updated. But for certain
     * operations we dont want to impact accessTime of the entries (like
     * periodic flush of current active logChannels), and those operations
     * can use this copy of references.
     */
    private final ConcurrentLongHashMap&lt;BufferedLogChannelWithDirInfo&gt; replicaOfCurrentLogChannels;
    private final CacheLoader&lt;Long, EntryLogAndLockTuple&gt; entryLogAndLockTupleCacheLoader;
    private final EntryLogger.RecentEntryLogsStatus recentlyCreatedEntryLogsStatus;
    private final int entrylogMapAccessExpiryTimeInSeconds;
    private final int maximumNumberOfActiveEntryLogs;
    private final int entryLogPerLedgerCounterLimitsMultFactor;

    // Expose Stats
    private final StatsLogger statsLogger;
    final EntryLogsPerLedgerCounter entryLogsPerLedgerCounter;

    EntryLogManagerForEntryLogPerLedger(ServerConfiguration conf, LedgerDirsManager ledgerDirsManager,
            EntryLoggerAllocator entryLoggerAllocator, List&lt;EntryLogger.EntryLogListener&gt; listeners,
            EntryLogger.RecentEntryLogsStatus recentlyCreatedEntryLogsStatus, StatsLogger statsLogger)
            throws IOException {
<span class="nc" id="L275">        super(conf, ledgerDirsManager, entryLoggerAllocator, listeners);</span>
<span class="nc" id="L276">        this.recentlyCreatedEntryLogsStatus = recentlyCreatedEntryLogsStatus;</span>
<span class="nc" id="L277">        this.rotatedLogChannels = new CopyOnWriteArrayList&lt;BufferedLogChannel&gt;();</span>
<span class="nc" id="L278">        this.replicaOfCurrentLogChannels = new ConcurrentLongHashMap&lt;BufferedLogChannelWithDirInfo&gt;();</span>
<span class="nc" id="L279">        this.entrylogMapAccessExpiryTimeInSeconds = conf.getEntrylogMapAccessExpiryTimeInSeconds();</span>
<span class="nc" id="L280">        this.maximumNumberOfActiveEntryLogs = conf.getMaximumNumberOfActiveEntryLogs();</span>
<span class="nc" id="L281">        this.entryLogPerLedgerCounterLimitsMultFactor = conf.getEntryLogPerLedgerCounterLimitsMultFactor();</span>

<span class="nc" id="L283">        ledgerDirsManager.addLedgerDirsListener(getLedgerDirsListener());</span>
<span class="nc" id="L284">        this.lockArrayPool = new AtomicReferenceArray&lt;Lock&gt;(maximumNumberOfActiveEntryLogs * 2);</span>
<span class="nc" id="L285">        this.entryLogAndLockTupleCacheLoader = new CacheLoader&lt;Long, EntryLogAndLockTuple&gt;() {</span>
            @Override
            public EntryLogAndLockTuple load(Long key) throws Exception {
<span class="nc" id="L288">                return new EntryLogAndLockTuple(key);</span>
            }
        };
        /*
         * Currently we are relying on access time based eviction policy for
         * removal of EntryLogAndLockTuple, so if the EntryLogAndLockTuple of
         * the ledger is not accessed in
         * entrylogMapAccessExpiryTimeInSeconds period, it will be removed
         * from the cache.
         *
         * We are going to introduce explicit advisory writeClose call, with
         * that explicit call EntryLogAndLockTuple of the ledger will be
         * removed from the cache. But still timebased eviciton policy is
         * needed because it is not guaranteed that Bookie/EntryLogger would
         * receive successfully write close call in all the cases.
         */
<span class="nc" id="L304">        ledgerIdEntryLogMap = CacheBuilder.newBuilder()</span>
<span class="nc" id="L305">                .expireAfterAccess(entrylogMapAccessExpiryTimeInSeconds, TimeUnit.SECONDS)</span>
<span class="nc" id="L306">                .maximumSize(maximumNumberOfActiveEntryLogs)</span>
<span class="nc" id="L307">                .removalListener(new RemovalListener&lt;Long, EntryLogAndLockTuple&gt;() {</span>
                    @Override
                    public void onRemoval(
                            RemovalNotification&lt;Long, EntryLogAndLockTuple&gt; expiredLedgerEntryLogMapEntry) {
<span class="nc" id="L311">                        onCacheEntryRemoval(expiredLedgerEntryLogMapEntry);</span>
<span class="nc" id="L312">                    }</span>
<span class="nc" id="L313">                }).build(entryLogAndLockTupleCacheLoader);</span>

<span class="nc" id="L315">        this.statsLogger = statsLogger;</span>
<span class="nc" id="L316">        this.entryLogsPerLedgerCounter = new EntryLogsPerLedgerCounter(this.statsLogger);</span>
<span class="nc" id="L317">    }</span>

    /*
     * This method is called when an entry is removed from the cache. This could
     * be because access time of that ledger has elapsed
     * entrylogMapAccessExpiryTimeInSeconds period, or number of active
     * currentlogs in the cache has reached the size of
     * maximumNumberOfActiveEntryLogs, or if an entry is explicitly
     * invalidated/removed. In these cases entry for that ledger is removed from
     * cache. Since the entrylog of this ledger is not active anymore it has to
     * be removed from replicaOfCurrentLogChannels and added to
     * rotatedLogChannels.
     *
     * Because of performance/optimizations concerns the cleanup maintenance
     * operations wont happen automatically, for more info on eviction cleanup
     * maintenance tasks -
     * https://google.github.io/guava/releases/19.0/api/docs/com/google/
     * common/cache/CacheBuilder.html
     *
     */
    private void onCacheEntryRemoval(RemovalNotification&lt;Long, EntryLogAndLockTuple&gt; removedLedgerEntryLogMapEntry) {
<span class="nc" id="L338">        Long ledgerId = removedLedgerEntryLogMapEntry.getKey();</span>
<span class="nc" id="L339">        log.debug(&quot;LedgerId {} is being evicted from the cache map because of {}&quot;, ledgerId,</span>
<span class="nc" id="L340">                removedLedgerEntryLogMapEntry.getCause());</span>
<span class="nc" id="L341">        EntryLogAndLockTuple entryLogAndLockTuple = removedLedgerEntryLogMapEntry.getValue();</span>
<span class="nc bnc" id="L342" title="All 2 branches missed.">        if (entryLogAndLockTuple == null) {</span>
<span class="nc" id="L343">            log.error(&quot;entryLogAndLockTuple is not supposed to be null in entry removal listener for ledger : {}&quot;,</span>
                    ledgerId);
<span class="nc" id="L345">            return;</span>
        }
<span class="nc" id="L347">        Lock lock = entryLogAndLockTuple.ledgerLock;</span>
<span class="nc" id="L348">        BufferedLogChannelWithDirInfo logChannelWithDirInfo = entryLogAndLockTuple.getEntryLogWithDirInfo();</span>
<span class="nc bnc" id="L349" title="All 2 branches missed.">        if (logChannelWithDirInfo == null) {</span>
<span class="nc" id="L350">            log.error(&quot;logChannel for ledger: {} is not supposed to be null in entry removal listener&quot;, ledgerId);</span>
<span class="nc" id="L351">            return;</span>
        }
<span class="nc" id="L353">        lock.lock();</span>
        try {
<span class="nc" id="L355">            BufferedLogChannel logChannel = logChannelWithDirInfo.getLogChannel();</span>
            // Append ledgers map at the end of entry log
            try {
<span class="nc" id="L358">                logChannel.appendLedgersMap();</span>
<span class="nc" id="L359">            } catch (Exception e) {</span>
<span class="nc" id="L360">                log.error(&quot;Got IOException while trying to appendLedgersMap in cacheEntryRemoval callback&quot;, e);</span>
<span class="nc" id="L361">            }</span>
<span class="nc" id="L362">            replicaOfCurrentLogChannels.remove(logChannel.getLogId());</span>
<span class="nc" id="L363">            rotatedLogChannels.add(logChannel);</span>
<span class="nc" id="L364">            entryLogsPerLedgerCounter.removedLedgerFromEntryLogMapCache(ledgerId,</span>
<span class="nc" id="L365">                    removedLedgerEntryLogMapEntry.getCause());</span>
        } finally {
<span class="nc" id="L367">            lock.unlock();</span>
        }
<span class="nc" id="L369">    }</span>

    private LedgerDirsListener getLedgerDirsListener() {
<span class="nc" id="L372">        return new LedgerDirsListener() {</span>
            @Override
            public void diskFull(File disk) {
<span class="nc" id="L375">                Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L376" title="All 2 branches missed.">                for (BufferedLogChannelWithDirInfo currentLogWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
<span class="nc bnc" id="L377" title="All 2 branches missed.">                    if (disk.equals(currentLogWithDirInfo.getLogChannel().getLogFile().getParentFile())) {</span>
<span class="nc" id="L378">                        currentLogWithDirInfo.setLedgerDirFull(true);</span>
                    }
<span class="nc" id="L380">                }</span>
<span class="nc" id="L381">            }</span>

            @Override
            public void diskWritable(File disk) {
<span class="nc" id="L385">                Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L386" title="All 2 branches missed.">                for (BufferedLogChannelWithDirInfo currentLogWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
<span class="nc bnc" id="L387" title="All 2 branches missed.">                    if (disk.equals(currentLogWithDirInfo.getLogChannel().getLogFile().getParentFile())) {</span>
<span class="nc" id="L388">                        currentLogWithDirInfo.setLedgerDirFull(false);</span>
                    }
<span class="nc" id="L390">                }</span>
<span class="nc" id="L391">            }</span>
        };
    }

    Lock getLock(long ledgerId) throws IOException {
        try {
<span class="nc" id="L397">            return ledgerIdEntryLogMap.get(ledgerId).getLedgerLock();</span>
<span class="nc" id="L398">        } catch (Exception e) {</span>
<span class="nc" id="L399">            log.error(&quot;Received unexpected exception while fetching lock to acquire for ledger: &quot; + ledgerId, e);</span>
<span class="nc" id="L400">            throw new IOException(&quot;Received unexpected exception while fetching lock to acquire&quot;, e);</span>
        }
    }

    /*
     * sets the logChannel for the given ledgerId. It will add the new
     * logchannel to replicaOfCurrentLogChannels, and the previous one will
     * be removed from replicaOfCurrentLogChannels. Previous logChannel will
     * be added to rotatedLogChannels in both the cases.
     */
    @Override
    public void setCurrentLogForLedgerAndAddToRotate(long ledgerId, BufferedLogChannel logChannel) throws IOException {
<span class="nc" id="L412">        Lock lock = getLock(ledgerId);</span>
<span class="nc" id="L413">        lock.lock();</span>
        try {
<span class="nc" id="L415">            BufferedLogChannel hasToRotateLogChannel = getCurrentLogForLedger(ledgerId);</span>
<span class="nc bnc" id="L416" title="All 2 branches missed.">            boolean newLedgerInEntryLogMapCache = (hasToRotateLogChannel == null);</span>
<span class="nc" id="L417">            logChannel.setLedgerIdAssigned(ledgerId);</span>
<span class="nc" id="L418">            BufferedLogChannelWithDirInfo logChannelWithDirInfo = new BufferedLogChannelWithDirInfo(logChannel);</span>
<span class="nc" id="L419">            ledgerIdEntryLogMap.get(ledgerId).setEntryLogWithDirInfo(logChannelWithDirInfo);</span>
<span class="nc" id="L420">            entryLogsPerLedgerCounter.openNewEntryLogForLedger(ledgerId, newLedgerInEntryLogMapCache);</span>
<span class="nc" id="L421">            replicaOfCurrentLogChannels.put(logChannel.getLogId(), logChannelWithDirInfo);</span>
<span class="nc bnc" id="L422" title="All 2 branches missed.">            if (hasToRotateLogChannel != null) {</span>
<span class="nc" id="L423">                replicaOfCurrentLogChannels.remove(hasToRotateLogChannel.getLogId());</span>
<span class="nc" id="L424">                rotatedLogChannels.add(hasToRotateLogChannel);</span>
            }
<span class="nc" id="L426">        } catch (Exception e) {</span>
<span class="nc" id="L427">            log.error(&quot;Received unexpected exception while fetching entry from map for ledger: &quot; + ledgerId, e);</span>
<span class="nc" id="L428">            throw new IOException(&quot;Received unexpected exception while fetching entry from map&quot;, e);</span>
        } finally {
<span class="nc" id="L430">            lock.unlock();</span>
        }
<span class="nc" id="L432">    }</span>

    @Override
    public BufferedLogChannel getCurrentLogForLedger(long ledgerId) throws IOException {
<span class="nc" id="L436">        BufferedLogChannelWithDirInfo bufferedLogChannelWithDirInfo = getCurrentLogWithDirInfoForLedger(ledgerId);</span>
<span class="nc" id="L437">        BufferedLogChannel bufferedLogChannel = null;</span>
<span class="nc bnc" id="L438" title="All 2 branches missed.">        if (bufferedLogChannelWithDirInfo != null) {</span>
<span class="nc" id="L439">            bufferedLogChannel = bufferedLogChannelWithDirInfo.getLogChannel();</span>
        }
<span class="nc" id="L441">        return bufferedLogChannel;</span>
    }

    public BufferedLogChannelWithDirInfo getCurrentLogWithDirInfoForLedger(long ledgerId) throws IOException {
<span class="nc" id="L445">        Lock lock = getLock(ledgerId);</span>
<span class="nc" id="L446">        lock.lock();</span>
        try {
<span class="nc" id="L448">            EntryLogAndLockTuple entryLogAndLockTuple = ledgerIdEntryLogMap.get(ledgerId);</span>
<span class="nc" id="L449">            return entryLogAndLockTuple.getEntryLogWithDirInfo();</span>
<span class="nc" id="L450">        } catch (Exception e) {</span>
<span class="nc" id="L451">            log.error(&quot;Received unexpected exception while fetching entry from map for ledger: &quot; + ledgerId, e);</span>
<span class="nc" id="L452">            throw new IOException(&quot;Received unexpected exception while fetching entry from map&quot;, e);</span>
        } finally {
<span class="nc" id="L454">            lock.unlock();</span>
        }
    }

    public Set&lt;BufferedLogChannelWithDirInfo&gt; getCopyOfCurrentLogs() {
<span class="nc" id="L459">        return new HashSet&lt;BufferedLogChannelWithDirInfo&gt;(replicaOfCurrentLogChannels.values());</span>
    }

    @Override
    public BufferedLogChannel getCurrentLogIfPresent(long entryLogId) {
<span class="nc" id="L464">        BufferedLogChannelWithDirInfo bufferedLogChannelWithDirInfo = replicaOfCurrentLogChannels.get(entryLogId);</span>
<span class="nc" id="L465">        BufferedLogChannel logChannel = null;</span>
<span class="nc bnc" id="L466" title="All 2 branches missed.">        if (bufferedLogChannelWithDirInfo != null) {</span>
<span class="nc" id="L467">            logChannel = bufferedLogChannelWithDirInfo.getLogChannel();</span>
        }
<span class="nc" id="L469">        return logChannel;</span>
    }

    @Override
    public void checkpoint() throws IOException {
        /*
         * In the case of entryLogPerLedgerEnabled we need to flush
         * both rotatedlogs and currentlogs. This is needed because
         * syncThread periodically does checkpoint and at this time
         * all the logs should be flushed.
         *
         */
<span class="nc" id="L481">        super.flush();</span>
<span class="nc" id="L482">    }</span>

    @Override
    public void prepareSortedLedgerStorageCheckpoint(long numBytesFlushed) throws IOException {
        // do nothing
        /*
         * prepareSortedLedgerStorageCheckpoint is required for
         * singleentrylog scenario, but it is not needed for
         * entrylogperledger scenario, since entries of a ledger go
         * to a entrylog (even during compaction) and SyncThread
         * drives periodic checkpoint logic.
         */

<span class="nc" id="L495">    }</span>

    @Override
    public void prepareEntryMemTableFlush() {
        // do nothing
<span class="nc" id="L500">    }</span>

    @Override
    public boolean commitEntryMemTableFlush() throws IOException {
        // lock it only if there is new data
        // so that cache accesstime is not changed
<span class="nc" id="L506">        Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L507" title="All 2 branches missed.">        for (BufferedLogChannelWithDirInfo currentLogWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
<span class="nc" id="L508">            BufferedLogChannel currentLog = currentLogWithDirInfo.getLogChannel();</span>
<span class="nc bnc" id="L509" title="All 2 branches missed.">            if (reachEntryLogLimit(currentLog, 0L)) {</span>
<span class="nc" id="L510">                Long ledgerId = currentLog.getLedgerIdAssigned();</span>
<span class="nc" id="L511">                Lock lock = getLock(ledgerId);</span>
<span class="nc" id="L512">                lock.lock();</span>
                try {
<span class="nc bnc" id="L514" title="All 2 branches missed.">                    if (reachEntryLogLimit(currentLog, 0L)) {</span>
<span class="nc" id="L515">                        log.info(&quot;Rolling entry logger since it reached size limitation for ledger: {}&quot;, ledgerId);</span>
<span class="nc" id="L516">                        createNewLog(ledgerId, &quot;after entry log file is rotated&quot;);</span>
                    }
                } finally {
<span class="nc" id="L519">                    lock.unlock();</span>
                }
            }
<span class="nc" id="L522">        }</span>
        /*
         * in the case of entrylogperledger, SyncThread drives
         * checkpoint logic for every flushInterval. So
         * EntryMemtable doesn't need to call checkpoint in the case
         * of entrylogperledger.
         */
<span class="nc" id="L529">        return false;</span>
    }

    /*
     * this is for testing purpose only. guava's cache doesnt cleanup
     * completely (including calling expiry removal listener) automatically
     * when access timeout elapses.
     *
     * https://google.github.io/guava/releases/19.0/api/docs/com/google/
     * common/cache/CacheBuilder.html
     *
     * If expireAfterWrite or expireAfterAccess is requested entries may be
     * evicted on each cache modification, on occasional cache accesses, or
     * on calls to Cache.cleanUp(). Expired entries may be counted by
     * Cache.size(), but will never be visible to read or write operations.
     *
     * Certain cache configurations will result in the accrual of periodic
     * maintenance tasks which will be performed during write operations, or
     * during occasional read operations in the absence of writes. The
     * Cache.cleanUp() method of the returned cache will also perform
     * maintenance, but calling it should not be necessary with a high
     * throughput cache. Only caches built with removalListener,
     * expireAfterWrite, expireAfterAccess, weakKeys, weakValues, or
     * softValues perform periodic maintenance.
     */
    @VisibleForTesting
    void doEntryLogMapCleanup() {
<span class="nc" id="L556">        ledgerIdEntryLogMap.cleanUp();</span>
<span class="nc" id="L557">    }</span>

    @VisibleForTesting
    ConcurrentMap&lt;Long, EntryLogAndLockTuple&gt; getCacheAsMap() {
<span class="nc" id="L561">        return ledgerIdEntryLogMap.asMap();</span>
    }
    /*
     * Returns writable ledger dir with least number of current active
     * entrylogs.
     */
    @Override
    public File getDirForNextEntryLog(List&lt;File&gt; writableLedgerDirs) {
<span class="nc" id="L569">        Map&lt;File, MutableInt&gt; writableLedgerDirFrequency = new HashMap&lt;File, MutableInt&gt;();</span>
<span class="nc" id="L570">        writableLedgerDirs.stream()</span>
<span class="nc" id="L571">                .forEach((ledgerDir) -&gt; writableLedgerDirFrequency.put(ledgerDir, new MutableInt()));</span>
<span class="nc bnc" id="L572" title="All 2 branches missed.">        for (BufferedLogChannelWithDirInfo logChannelWithDirInfo : replicaOfCurrentLogChannels.values()) {</span>
<span class="nc" id="L573">            File parentDirOfCurrentLogChannel = logChannelWithDirInfo.getLogChannel().getLogFile().getParentFile();</span>
<span class="nc bnc" id="L574" title="All 2 branches missed.">            if (writableLedgerDirFrequency.containsKey(parentDirOfCurrentLogChannel)) {</span>
<span class="nc" id="L575">                writableLedgerDirFrequency.get(parentDirOfCurrentLogChannel).increment();</span>
            }
<span class="nc" id="L577">        }</span>
        @SuppressWarnings(&quot;unchecked&quot;)
<span class="nc" id="L579">        Optional&lt;Entry&lt;File, MutableInt&gt;&gt; ledgerDirWithLeastNumofCurrentLogs = writableLedgerDirFrequency.entrySet()</span>
<span class="nc" id="L580">                .stream().min(Map.Entry.comparingByValue());</span>
<span class="nc" id="L581">        return ledgerDirWithLeastNumofCurrentLogs.get().getKey();</span>
    }

    @Override
    public void close() throws IOException {
<span class="nc" id="L586">        Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L587" title="All 2 branches missed.">        for (BufferedLogChannelWithDirInfo currentLogWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
<span class="nc bnc" id="L588" title="All 2 branches missed.">            if (currentLogWithDirInfo.getLogChannel() != null) {</span>
<span class="nc" id="L589">                currentLogWithDirInfo.getLogChannel().close();</span>
            }
<span class="nc" id="L591">        }</span>
<span class="nc" id="L592">    }</span>

    @Override
    public void forceClose() {
<span class="nc" id="L596">        Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L597" title="All 2 branches missed.">        for (BufferedLogChannelWithDirInfo currentLogWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
<span class="nc" id="L598">            IOUtils.close(log, currentLogWithDirInfo.getLogChannel());</span>
<span class="nc" id="L599">        }</span>
<span class="nc" id="L600">    }</span>

    @Override
    void flushCurrentLogs() throws IOException {
<span class="nc" id="L604">        Set&lt;BufferedLogChannelWithDirInfo&gt; copyOfCurrentLogsWithDirInfo = getCopyOfCurrentLogs();</span>
<span class="nc bnc" id="L605" title="All 2 branches missed.">        for (BufferedLogChannelWithDirInfo logChannelWithDirInfo : copyOfCurrentLogsWithDirInfo) {</span>
            /**
             * flushCurrentLogs method is called during checkpoint, so metadata
             * of the file also should be force written.
             */
<span class="nc" id="L610">            flushLogChannel(logChannelWithDirInfo.getLogChannel(), true);</span>
<span class="nc" id="L611">        }</span>
<span class="nc" id="L612">    }</span>

    @Override
    public BufferedLogChannel createNewLogForCompaction() throws IOException {
<span class="nc" id="L616">        throw new UnsupportedOperationException(</span>
                &quot;When entryLogPerLedger is enabled, transactional compaction should have been disabled&quot;);
    }

    @Override
    public long addEntry(long ledger, ByteBuf entry, boolean rollLog) throws IOException {
<span class="nc" id="L622">        Lock lock = getLock(ledger);</span>
<span class="nc" id="L623">        lock.lock();</span>
        try {
<span class="nc" id="L625">            return super.addEntry(ledger, entry, rollLog);</span>
        } finally {
<span class="nc" id="L627">            lock.unlock();</span>
        }
    }

    @Override
    void createNewLog(long ledgerId) throws IOException {
<span class="nc" id="L633">        Lock lock = getLock(ledgerId);</span>
<span class="nc" id="L634">        lock.lock();</span>
        try {
<span class="nc" id="L636">            super.createNewLog(ledgerId);</span>
        } finally {
<span class="nc" id="L638">            lock.unlock();</span>
        }
<span class="nc" id="L640">    }</span>


    @Override
    BufferedLogChannel getCurrentLogForLedgerForAddEntry(long ledgerId, int entrySize, boolean rollLog)
            throws IOException {
<span class="nc" id="L646">        Lock lock = getLock(ledgerId);</span>
<span class="nc" id="L647">        lock.lock();</span>
        try {
<span class="nc" id="L649">            BufferedLogChannelWithDirInfo logChannelWithDirInfo = getCurrentLogWithDirInfoForLedger(ledgerId);</span>
<span class="nc" id="L650">            BufferedLogChannel logChannel = null;</span>
<span class="nc bnc" id="L651" title="All 2 branches missed.">            if (logChannelWithDirInfo != null) {</span>
<span class="nc" id="L652">                logChannel = logChannelWithDirInfo.getLogChannel();</span>
            }
<span class="nc bnc" id="L654" title="All 2 branches missed.">            boolean reachEntryLogLimit = rollLog ? reachEntryLogLimit(logChannel, entrySize)</span>
<span class="nc" id="L655">                    : readEntryLogHardLimit(logChannel, entrySize);</span>
            // Create new log if logSizeLimit reached or current disk is full
<span class="nc bnc" id="L657" title="All 2 branches missed.">            boolean diskFull = (logChannel == null) ? false : logChannelWithDirInfo.isLedgerDirFull();</span>
<span class="nc bnc" id="L658" title="All 2 branches missed.">            boolean allDisksFull = !ledgerDirsManager.hasWritableLedgerDirs();</span>

            /**
             * if disk of the logChannel is full or if the entrylog limit is
             * reached of if the logchannel is not initialized, then
             * createNewLog. If allDisks are full then proceed with the current
             * logChannel, since Bookie must have turned to readonly mode and
             * the addEntry traffic would be from GC and it is ok to proceed in
             * this case.
             */
<span class="nc bnc" id="L668" title="All 8 branches missed.">            if ((diskFull &amp;&amp; (!allDisksFull)) || reachEntryLogLimit || (logChannel == null)) {</span>
<span class="nc bnc" id="L669" title="All 2 branches missed.">                if (logChannel != null) {</span>
<span class="nc" id="L670">                    logChannel.flushAndForceWriteIfRegularFlush(false);</span>
                }
<span class="nc" id="L672">                createNewLog(ledgerId,</span>
                    &quot;: diskFull = &quot; + diskFull + &quot;, allDisksFull = &quot; + allDisksFull
                        + &quot;, reachEntryLogLimit = &quot; + reachEntryLogLimit + &quot;, logChannel = &quot; + logChannel);
            }

<span class="nc" id="L677">            return getCurrentLogForLedger(ledgerId);</span>
        } finally {
<span class="nc" id="L679">            lock.unlock();</span>
        }
    }

    @Override
    public void flushRotatedLogs() throws IOException {
<span class="nc bnc" id="L685" title="All 2 branches missed.">        for (BufferedLogChannel channel : rotatedLogChannels) {</span>
<span class="nc" id="L686">            channel.flushAndForceWrite(true);</span>
            // since this channel is only used for writing, after flushing the channel,
            // we had to close the underlying file channel. Otherwise, we might end up
            // leaking fds which cause the disk spaces could not be reclaimed.
<span class="nc" id="L690">            channel.close();</span>
<span class="nc" id="L691">            recentlyCreatedEntryLogsStatus.flushRotatedEntryLog(channel.getLogId());</span>
<span class="nc" id="L692">            rotatedLogChannels.remove(channel);</span>
<span class="nc" id="L693">            log.info(&quot;Synced entry logger {} to disk.&quot;, channel.getLogId());</span>
<span class="nc" id="L694">        }</span>
<span class="nc" id="L695">    }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.4.201905082037</span></div></body></html>